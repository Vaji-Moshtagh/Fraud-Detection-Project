{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "This notebook demonstrates how to build and evaluate machine learning models for credit card fraud detection. \n",
    "\n",
    "We'll explore:\n",
    "1. Understanding the data and class imbalance\n",
    "2. Building a baseline model\n",
    "3. Handling class imbalance with SMOTE\n",
    "4. Using class weights\n",
    "5. Evaluating models with appropriate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom modules\n",
    "from model import FraudDetector\n",
    "from utils import (\n",
    "    calculate_class_distribution,\n",
    "    preprocess_data,\n",
    "    plot_roc_curves,\n",
    "    plot_confusion_matrices,\n",
    "    plot_feature_importance\n",
    ")\n",
    "\n",
    "# Set some plotting parameters\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data\n",
    "\n",
    "For this example, we'll use synthetic data to mimic credit card transactions. In a real scenario, you would load your actual dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a synthetic dataset\n",
    "def create_synthetic_data(n_samples=10000, n_features=10, fraud_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset to mimic credit card transactions.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of transactions\n",
    "    - n_features: Number of features (will be named V1, V2, etc.)\n",
    "    - fraud_ratio: Ratio of fraudulent transactions\n",
    "    \"\"\"\n",
    "    # Create feature names\n",
    "    feature_names = [f'V{i+1}' for i in range(n_features)]\n",
    "    \n",
    "    # Generate non-fraudulent transactions (normally distributed)\n",
    "    n_non_fraud = int(n_samples * (1 - fraud_ratio))\n",
    "    non_fraud_data = np.random.normal(0, 1, size=(n_non_fraud, n_features))\n",
    "    non_fraud_labels = np.zeros(n_non_fraud)\n",
    "    \n",
    "    # Generate fraudulent transactions (different distribution)\n",
    "    n_fraud = int(n_samples * fraud_ratio)\n",
    "    fraud_data = np.random.normal(-2, 2, size=(n_fraud, n_features))\n",
    "    fraud_labels = np.ones(n_fraud)\n",
    "    \n",
    "    # Combine the data\n",
    "    X = np.vstack([non_fraud_data, fraud_data])\n",
    "    y = np.hstack([non_fraud_labels, fraud_labels])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['Class'] = y.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "data = create_synthetic_data(n_samples=10000, n_features=10, fraud_ratio=0.01)\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explore class distribution\n",
    "class_stats = calculate_class_distribution(data['Class'])\n",
    "\n",
    "# Display statistics\n",
    "for key, value in class_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Plot the class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Class', data=data)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0: Non-Fraud, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "\n",
    "Next, we'll prepare our data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "X, y = preprocess_data(data, target_col='Class')\n",
    "\n",
    "# Check the processed data\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate Models\n",
    "\n",
    "Now we'll train various models to detect fraud and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize our fraud detector\n",
    "detector = FraudDetector(random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = detector.prepare_data(X, y)\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = detector.train_baseline_model(X_train, y_train)\n",
    "\n",
    "# Train model with SMOTE\n",
    "smote_model = detector.train_smote_model(X_train, y_train)\n",
    "\n",
    "# Train model with class weights\n",
    "weighted_model = detector.train_weighted_model(X_train, y_train, weight_ratio=100)\n",
    "\n",
    "# Train logistic regression model\n",
    "logistic_model = detector.train_logistic_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in detector.models.items():\n",
    "    results[name] = detector.evaluate_model(model, X_test, y_test)\n",
    "    print(f\"\\n--- {name.upper()} MODEL EVALUATION ---\")\n",
    "    print(f\"AUC: {results[name]['auc']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(results[name]['confusion_matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    for cls in ['0', '1']:\n",
    "        cr = results[name]['classification_report'][cls]\n",
    "        print(f\"Class {cls} - Precision: {cr['precision']:.4f}, \"\n",
    "              f\"Recall: {cr['recall']:.4f}, \"\n",
    "              f\"F1: {cr['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "Let's create some visualizations to better understand our models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROC curves\n",
    "plot_roc_curves(results)\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(results)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrices\n",
    "plot_confusion_matrices(results, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feature importance for baseline model\n",
    "importance = detector.get_feature_importance('baseline')\n",
    "plt_fig, feat_df = plot_feature_importance(X.columns, importance)\n",
    "plt.show()\n",
    "\n",
    "# Display top features\n",
    "feat_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook demonstrated several key concepts in fraud detection:\n",
    "\n",
    "1. **Understanding class imbalance**: We saw that fraud is typically a rare event, creating challenges for standard ML approaches.\n",
    "\n",
    "2. **Handling imbalanced data**: We explored multiple techniques:\n",
    "   - Baseline model (no special handling)\n",
    "   - SMOTE for synthetic minority oversampling\n",
    "   - Class weighting\n",
    "   - Alternative algorithms (logistic regression)\n",
    "\n",
    "3. **Appropriate evaluation**: We used metrics beyond accuracy:\n",
    "   - ROC curves and AUC\n",
    "   - Precision and recall \n",
    "   - Confusion matrices\n",
    "\n",
    "In a real-world scenario, additional steps would include:\n",
    "- Hyperparameter tuning\n",
    "- More advanced feature engineering\n",
    "- Deployment considerations\n",
    "- Monitoring for model drift\n",
    "\n",
    "For a production fraud detection system, you would likely combine multiple models and business rules to achieve optimal performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
